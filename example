import gradio as gr
import torchaudio
import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutomaticSpeechRecognitionPipeline
import numpy as np
import tempfile
import os

# å…¨åŸŸè®Šæ•¸å­˜å„²æ¨¡å‹
processor = None
model = None
asr_pipeline = None

def load_model():
    """è¼‰å…¥ Breeze ASR 25 æ¨¡å‹"""
    global processor, model, asr_pipeline
    
    try:
        processor = WhisperProcessor.from_pretrained("MediaTek-Research/Breeze-ASR-25")
        model = WhisperForConditionalGeneration.from_pretrained("MediaTek-Research/Breeze-ASR-25")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ CUDA
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device).eval()
        
        # å»ºç«‹ pipeline
        asr_pipeline = AutomaticSpeechRecognitionPipeline(
            model=model,
            tokenizer=processor.tokenizer,
            feature_extractor=processor.feature_extractor,
            chunk_length_s=0,
            device=device
        )
        
        return f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ä½¿ç”¨è¨­å‚™: {device}"
    except Exception as e:
        return f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}"

def preprocess_audio(audio_path):
    """éŸ³è¨Šé è™•ç†"""
    # è¼‰å…¥éŸ³è¨Š
    waveform, sample_rate = torchaudio.load(audio_path)
    
    # è½‰ç‚ºå–®è²é“
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0)
    
    waveform = waveform.squeeze().numpy()
    
    # é‡æ¡æ¨£åˆ° 16kHz
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(sample_rate, 16000)
        waveform = resampler(torch.tensor(waveform)).numpy()
    
    return waveform

def transcribe_audio(audio_input):
    """èªéŸ³è¾¨è­˜ä¸»å‡½æ•¸"""
    global asr_pipeline
    
    try:
        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²è¼‰å…¥
        if asr_pipeline is None:
            status = load_model()
            if "å¤±æ•—" in status:
                return status, "", "", ""
        
        # æª¢æŸ¥éŸ³è¨Šè¼¸å…¥
        if audio_input is None:
            return "âŒ è«‹å…ˆä¸Šå‚³éŸ³è¨Šæª”æ¡ˆæˆ–é€²è¡ŒéŒ„éŸ³", "", "", ""
        
        # è™•ç†ä¸åŒçš„éŸ³è¨Šè¼¸å…¥æ ¼å¼
        if isinstance(audio_input, str):
            # æª”æ¡ˆè·¯å¾‘
            audio_path = audio_input
        elif isinstance(audio_input, tuple):
            # Gradio éŒ„éŸ³æ ¼å¼ (sample_rate, audio_data)
            sample_rate, audio_data = audio_input
            
            # å»ºç«‹è‡¨æ™‚æª”æ¡ˆ
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                # ç¢ºä¿éŸ³è¨Šæ•¸æ“šæ ¼å¼æ­£ç¢º
                if audio_data.dtype != np.float32:
                    audio_data = audio_data.astype(np.float32)
                
                # æ­£è¦åŒ–éŸ³è¨Š
                if audio_data.max() > 1.0:
                    audio_data = audio_data / 32768.0
                
                # å„²å­˜ç‚º wav æª”æ¡ˆ
                torchaudio.save(tmp_file.name, torch.tensor(audio_data).unsqueeze(0), sample_rate)
                audio_path = tmp_file.name
        else:
            return "âŒ ä¸æ”¯æ´çš„éŸ³è¨Šæ ¼å¼", "", "", ""
        
        # é è™•ç†éŸ³è¨Š
        waveform = preprocess_audio(audio_path)
        
        # åŸ·è¡ŒèªéŸ³è¾¨è­˜
        result = asr_pipeline(waveform, return_timestamps=True)
        
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        if isinstance(audio_input, tuple) and os.path.exists(audio_path):
            os.unlink(audio_path)
        
        # æ ¼å¼åŒ–çµæœ
        transcription = result["text"].strip()
        
        # æ ¼å¼åŒ–æ™‚é–“æˆ³è¨˜é¡¯ç¤º
        formatted_text = ""
        pure_text = ""
        srt_text = ""
        
        if "chunks" in result and result["chunks"]:
            for i, chunk in enumerate(result["chunks"], 1):
                start_time = chunk["timestamp"][0] if chunk["timestamp"][0] is not None else 0
                end_time = chunk["timestamp"][1] if chunk["timestamp"][1] is not None else 0
                text = chunk['text'].strip()
                
                if text:  # åªè™•ç†éç©ºæ–‡å­—
                    # æ ¼å¼åŒ–é¡¯ç¤ºæ–‡å­—
                    #formatted_text += f"[{start_time:.2f}s - {end_time:.2f}s]: {text}\n"
                    
                    # ç´”æ–‡å­—ï¼ˆä¸å«æ™‚é–“æˆ³è¨˜ï¼‰
                    pure_text += f"{text}\n"
                    
                    # SRT æ ¼å¼
                    start_srt = f"{int(start_time//3600):02d}:{int((start_time%3600)//60):02d}:{int(start_time%60):02d},{int((start_time%1)*1000):03d}"
                    end_srt = f"{int(end_time//3600):02d}:{int((end_time%3600)//60):02d}:{int(end_time%60):02d},{int((end_time%1)*1000):03d}"
                    srt_text += f"{i}\n{start_srt} --> {end_srt}\n{text}\n\n"
        else:
            # å¦‚æœæ²’æœ‰æ™‚é–“æˆ³è¨˜ï¼Œåªé¡¯ç¤ºæ–‡å­—
            #formatted_text = transcription
            pure_text = transcription
            srt_text = f"1\n00:00:00,000 --> 00:00:10,000\n{transcription}\n\n"
        
        return "âœ… è¾¨è­˜å®Œæˆ", pure_text.strip(), srt_text.strip()
        
    except Exception as e:
        return f"âŒ è¾¨è­˜éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}", ""

def clear_all():
    """æ¸…é™¤æ‰€æœ‰å…§å®¹"""
    return None, "ğŸ”„ å·²æ¸…é™¤æ‰€æœ‰å…§å®¹", "", "", ""

# å»ºç«‹ Gradio ä»‹é¢
with gr.Blocks(title="èªéŸ³è¾¨è­˜ç³»çµ±", theme=gr.themes.Soft()) as demo:
    
    gr.Markdown("""
    # ğŸ¤ èªéŸ³è¾¨è­˜ç³»çµ± - Breeze ASR 25
    
    ### åŠŸèƒ½ç‰¹è‰²ï¼š
    - ğŸ”§ ä½¿ç”¨ Breeze ASR 25 æ¨¡å‹ï¼Œå°ˆç‚ºç¹é«”ä¸­æ–‡å„ªåŒ–
    - â° é¡¯ç¤ºæ™‚é–“æˆ³è¨˜
    - ğŸŒ å¼·åŒ–ä¸­è‹±æ··ç”¨è¾¨è­˜èƒ½åŠ›
    - æ„Ÿè¬[MediaTek-Research/Breeze-ASR-25](https://huggingface.co/MediaTek-Research/Breeze-ASR-25)
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            # éŸ³è¨Šè¼¸å…¥å€åŸŸ
            gr.Markdown("### ğŸ“‚ éŸ³è¨Šè¼¸å…¥")
            
            with gr.Tab("æª”æ¡ˆä¸Šå‚³"):
                audio_file = gr.Audio(
                    sources=["upload"],
                    label="ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆ",
                    type="filepath"
                )
            
            
            # æ§åˆ¶æŒ‰éˆ•
            with gr.Row():
                transcribe_btn = gr.Button("ğŸš€ é–‹å§‹è¾¨è­˜", variant="primary", size="lg")
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤", variant="secondary")
        
        with gr.Column(scale=1):
            # ç‹€æ…‹é¡¯ç¤º
            status_output = gr.Textbox(
                label="ğŸ“Š ç‹€æ…‹",
                placeholder="ç­‰å¾…æ“ä½œ...",
                interactive=False,
                lines=2
            )
            
            
            # ç´”æ–‡å­—çµæœ
            pure_text_output = gr.Textbox(
                label="ğŸ“„ ç´”æ–‡å­—çµæœ",
                placeholder="ç´”æ–‡å­—çµæœ...",
                lines=4,
                max_lines=10,
                show_copy_button=True
            )
            
            # SRT å­—å¹•æ ¼å¼
            srt_output = gr.Textbox(
                label="ğŸ¬ SRT å­—å¹•æ ¼å¼",
                placeholder="SRT æ ¼å¼å­—å¹•...",
                lines=6,
                max_lines=15,
                show_copy_button=True
            )
    
    
    # ä¿®æ­£äº‹ä»¶ç¶å®š
    def transcribe_wrapper(audio_file_val, audio_mic_val):
        audio_input = audio_file_val if audio_file_val else audio_mic_val
        return transcribe_audio(audio_input)
    
    transcribe_btn.click(
        fn=transcribe_wrapper,
        inputs=[audio_file],
        outputs=[status_output,  pure_text_output, srt_output]
    )
    
    clear_btn.click(
        fn=clear_all,
        outputs=[audio_file, status_output, pure_text_output, srt_output]
    )

# å•Ÿå‹•æ‡‰ç”¨
if __name__ == "__main__":
    demo.launch()